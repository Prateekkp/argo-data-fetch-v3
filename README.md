# ARGO Ocean Data Processing Workflow

A comprehensive Python workflow for downloading, processing, and analyzing ARGO oceanographic float data from NOAA servers. This project provides an automated 7-step pipeline to convert raw ARGO data into structured Parquet files for analysis.

## ğŸŒŠ Features

- **Automated Data Pipeline**: Complete 7-step workflow from data discovery to processing
- **Parallel Downloads**: High-performance concurrent downloading with resume capability
- **Data Validation**: Robust error handling and data quality checks
- **Database Integration**: PostgreSQL storage for efficient data indexing and querying
- **Flexible Output**: Converts NetCDF to Parquet format for analysis
- **Beginner Friendly**: Simple interface - just input region and year

## ğŸš€ Quick Start

### Prerequisites

- Python 3.7+
- PostgreSQL database
- Internet connection for data downloads

### Installation

1. **Clone the repository**:
   ```bash
   git clone <your-repo-url>
   cd "Data Fetching v3"
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables**:
   ```bash
   copy .env.example .env
   ```
   Edit `.env` with your database credentials:
   ```env
   DB_HOST=localhost
   DB_NAME=argo_index
   DB_USER=postgres
   DB_PASSWORD=your_password_here
   DB_PORT=5432
   ```

4. **Create PostgreSQL database**:
   ```sql
   CREATE DATABASE argo_index;
   ```

5. **Run the workflow**:
   ```bash
   python main_workflow.py
   ```

## ğŸ“Š Usage

The workflow supports various ocean regions:
- `atlantic` - Atlantic Ocean
- `pacific` - Pacific Ocean 
- `indian` - Indian Ocean
- `arctic` - Arctic Ocean
- `southern` - Southern Ocean

Simply run the main script and follow the prompts:

```bash
python main_workflow.py
```

**Example interaction**:
```
Enter ocean region (atlantic/pacific/indian): atlantic
Enter year (2000-2024): 2020
```

## What this workflow does:

1. **Index Download** - Downloads ARGO index files for the specified region and year
2. **Database Save** - Saves index data to PostgreSQL database  
3. **File Existence Check** - Checks which NetCDF files exist and downloads missing ones
4. **NetCDF Download** - Downloads all required NetCDF float data files
5. **Data Conversion** - Converts NetCDF files to structured Parquet format
6. **Parquet Merge** - Combines all data into one final file (region & year specific)
7. **Final Inspection** - Shows summary and sample of the processed data

## ğŸ“ Project Structure

```
ARGO Data Processing/
â”œâ”€â”€ main_workflow.py           # Main script - run this!
â”œâ”€â”€ database_setup.py          # Database configuration - edit this first!
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ modules/                   # Processing modules (don't edit these)
â”‚   â”œâ”€â”€ index_file_download.py
â”‚   â”œâ”€â”€ db_config.py
â”‚   â”œâ”€â”€ file_exisitance_checking.py
â”‚   â”œâ”€â”€ netcdf_file_dowload.py
â”‚   â”œâ”€â”€ data_loading_module.py
â”‚   â”œâ”€â”€ parquet_merger_module.py
â”‚   â””â”€â”€ parquet_opener_module.py
â”œâ”€â”€ argo_index/               # Downloaded index files
â”œâ”€â”€ argo_float_data/          # Downloaded NetCDF files
â””â”€â”€ processed_data/           # Final parquet files
    â”œâ”€â”€ {region}/             # Batch files by region
    â””â”€â”€ merged/{region}/      # Final merged files by region
```

## âš™ï¸ Configuration

### Database Setup
Edit `database_setup.py` with your PostgreSQL settings:
- Host, database name, username, password
- Table name for storing index data
- Connection settings

### Advanced Usage
You can run individual modules if needed, but the main workflow handles everything automatically.

## ğŸ“‹ Requirements

- Python 3.7+
- PostgreSQL database
- Required Python packages:
  - requests, beautifulsoup4, psycopg2
  - aiohttp, aiofiles
  - xarray, pandas, dask

## ğŸ¯ Output

After running the workflow for region="indian" and year="2020":
- **Batch files**: `processed_data/indian/indian_2020_batch*.parquet`
- **Final file**: `processed_data/merged/indian/indian_2020_full.parquet`

## âœ¨ Features

âœ… **One-click setup** - Edit database config once, run anywhere  
âœ… **Interactive interface** - Simple menu-driven workflow  
âœ… **Organized output** - Clean folder structure by region  
âœ… **Robust downloads** - Handles network issues gracefully  
âœ… **Progress tracking** - Shows progress at each step  
âœ… **Resumable** - Can restart if interrupted